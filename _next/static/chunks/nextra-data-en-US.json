{"/0-setup-access-tools/0.1":{"title":"1. Administrator 권한의 IAM User, Access Key, Access Key Secret 생성","data":{}},"/0-setup-access-tools/0.2":{"title":"AWS Cloud 9 셋업","data":{}},"/0-setup-access-tools/0.4":{"title":"4. Cloud9 의 Access Key, Access Key Secret 새로고침 문제 해결방법","data":{}},"/0-setup-access-tools/0.3":{"title":"3. EKS, k8s 접근 도구 셋업","data":{}},"/1-create-an-eks/1.2":{"title":"1.2","data":{"2-eksctl-을-사용해-eks-nodegroup-생성#2. eksctl 을 사용해 EKS Nodegroup 생성":"이미 배포되어있는 m5.large 타입의 인스턴스 2기를 t3.small 로 교체해주려고 하는데, 항상 그렇듯 물리적으로 한번에 교체하는 것은 불가능합니다. 이미 구동중인 m5.large 는 그대로 둔 상태에서 t3.small 기반의 클러스터를 배포합니다. 그리고 남아있는 m5.large 인스턴스 두기는 삭제합니다. 이번 문서에서는 t3.small 인스턴스 2 기를 배포하는 과정을 설명합니다.\r\nminimum-cluster.yml 이라는 이름의 파일을 생성하고 아래의 yml 명세서를 작성합니다.\n---\r\napiVersion: eksctl.io/v1alpha5\r\nkind: ClusterConfig\r\nmetadata:\r\n  name: gitops-study-k8scluster\r\n  region: ap-northeast-2\r\nmanagedNodeGroups:\r\n  - name: ng-workers\r\n    labels:\r\n      role: workers\r\n    instanceType: t3.small\r\n    desiredCapacity: 2\r\n    volumeSize: 20\r\n    privateNetworking: true\n그리고 아래의 eksctl 커맨드를 실행합니다.\neksctl create nodegroup --config-file=minimum-cluster.yml\nCloud 9 내에서 적용한 모습\r\n생성중인 노드는 콘솔에서 확인 가능합니다.\r\n\r\n새로 생성되는 중인 노드그룹은 ng-workers 입니다. ng-99fc00b4 라는 이름으로 이미 생성중인 노드 그룹은 노드의 인스턴스 크기가 커서 비용과금될 가능성이 크기 때문에 삭제할 예정입니다.\r\nng-workers 를 새로 만든 후에 ng-99fc00b4 을 삭제하는 이유는 이미 만들어둔 ng-99fc00b4 을 생성하면서 배포되어 있는 시스템 리소스들이 존재하는데 이 리소스들이 기존의 ec2 노드들에 자리잡고 있다. 그런데 이것을 삭제해버리면 이 노드들이 죽은 상태로 존재하게 됩니다.\r\n따라서 ng-workers 를 새로 추가 후에 ng-99fc00b4 를 삭제하면 기존에 생성된 노드들이 ng-workers 로 자연스럽게 이동이 될수 있기에 생성 후 삭제를 선택했습니다."}},"/1-create-an-eks/1.0":{"title":"1.0","data":{"소개#소개":"개인적으로는 EKS 를 CloudFormation 을 통해서 만드는 것을 추천하는 편입니다.\r\n하지만 처음부터 CloudFormation 으로 만드는 것은 쉽지는 않고, 먼저 콘솔에서 EKS 를 생성한 후에\r\n리소스들을 어떤 것들을 만들었는지 이런 것들을 모두 나열해보고 정리한 후에 CloudFormation 으로 EKS 를 생성하는 경우가 많지 않을까 싶습니다.이번문서는 콘솔에서 EKS 를 만드는 예를 정리합니다."}},"/1-create-an-eks/1.4":{"title":"1.4","data":{"4-eks-추가기능-vpc-cni-csi-드라이버-kube-proxy-coredns-설치#4. EKS 추가기능 (VPC CNI, CSI 드라이버, kube-proxy, CoreDNS) 설치":"설치할 추가 기능들은 아래와 같습니다.\nAmazon VPC CNI\n클러스터 내에서의 Pod 의 네트워킹에 관련된 기능\nEKS 클러스터가 EC2 노드에 pod 을 배포할 때 네트워크를 어떻게 사용할 것인가에 대한 내\n기본적으로 아마존에서 제공하는 VPC의 네트워크 인터페이스에 대해서는 Elastic Network Interface 를 사용하도록 되어 있다. Elastic Network Interface 는 가상의 네트워크 카드 역할을 하는 특정 기능을 의미한다.\nAmazon VPC CNI 는 EKS 클러스터가 EC2 노드에 pod 을 배포할 때 ENI(Elastic Network Interface)를 사용하도록 도와주는 추가기능\nAmazon EBS CSI 드라이버\n클러스터 내에서 Amazon Elastic Block Storage(EBS)를 활성화한다.\nEKS 클러스터가 PV를 사용할 때, EBS 볼륨을 활용할 수 있도록 해주는 드라이버\n클러스터에 파드를 배포할 때 단순히 클러스터가 살아있는 동안에만 파드가 스토리지를 유지하도록 하는 경우도 있지만, 파드가 수행하는 작업들이 영구적인 디스크를 사용해야 하는 경우가 있는데 이런 경우에 PV를 사용한다. Amazon EBS CSI 드라이버는 PV 사용시에 볼륨을 EBS 볼륨으로 마운트할 수 있도록 도와주는 기능이다.\nkube-proxy\n클러스터 내에서의 Pod 의 네트워킹에 관련된 기능\nPod 과 같은 리소스가 배포되는 EC2 워커노드와 Pod 간 네트워크 통신 기능을 담당하는 기능\n각 노드마다 kube-proxy 라는 이름의 파드가 배치된다\n노드는 물리적인 vm이라고 볼수 있고 pod 는 하나의 컨테이너라고 볼수 있는데, 배포된 여러가지 서비스 애플리케이션 pod 들이 다른 pod 과 네트워크 통신을 할 때 현재 배포된 노드 내의 kube-proxy 를 활용해 통신한다.\nCoreDNS\n클러스터 내에서 서비스 검색 시 DNS를 통한 검색이 가능하게끔 도와주는 기능\nEKS 클러스터 내에서 DNS(도메인 네임 서비스) 역할을 하게끔 도와주는 기능\n배포되는 모든 Pod 에 대해 도메인 이름 확인할 수 있는 기능이 제공된다.\nEKS 대시보드 진입 → 추가기능 → 추가기능 가져오기 클릭\r\n나타나는 화면에서 kube-proxy, Amazon VPC CNI, CoreDNS, Amazon EBS CSI 드라이버 를 선택한 후 다음 버튼을 누른다.\r\n설치할 확장기능들의 버전이나 IAM 역할 등을 선택하는 화면이 나오는데, 기본설정을 그대로 유지한 채로 다음 버튼 클릭\r\n확인화면이다. 별다른 이상이 없기에 생성 버튼을 눌러서 추가기능 설치를 마무리한다."}},"/1-create-an-eks/1.3":{"title":"1.3","data":{"3-managed-nodegroup-삭제#3. Managed Nodegroup 삭제":"이미 구동중이던 Managed Node Group 을 삭제하는 것은 아래의 명령어를 수행하면 됩니다.","노드그룹-삭제#노드그룹 삭제":"eksctl delete nodegroup --cluster=gitops-study-k8scluster --name=ng-99fc00b4","노드-조회#노드 조회":"kubectl get no\r\nNAME                                                 STATUS                        ROLES    AGE   VERSION\r\nip-192-168-105-26.ap-northeast-2.compute.internal    Ready                         <none>   11m   v1.27.7-eks-e71965b\r\nip-192-168-164-248.ap-northeast-2.compute.internal   Ready                         <none>   11m   v1.27.7-eks-e71965b\r\nip-192-168-28-115.ap-northeast-2.compute.internal    Ready,SchedulingDisabled      <none>   67m   v1.27.7-eks-e71965b\r\nip-192-168-38-181.ap-northeast-2.compute.internal    NotReady,SchedulingDisabled   <none>   67m   v1.27.7-eks-e71965b\n새로 생성 중인 노드를 확인해보면 아래와 같습니다."}},"/1-create-an-eks/1.1":{"title":"1.1","data":{"1-eksctl-을-사용해-eks-cluster-생성#1. eksctl 을 사용해 EKS Cluster 생성":"eksctl 을 이용해 gitops-study-k8scluster 라는 이름의 클러스터를 생성한다.\neksctl create cluster --name gitops-study-k8scluster --region ap-northeast-2\n생성중인 모습 (Cloud 9)\r\n생성 완료\r\n클러스터를 생성하면 클러스터에 대한 VPC 도 하나 생성된다.\r\n(매니지먼트 콘솔 → VPC 메뉴)\r\n클러스터에 대한 서브넷이 생성된 모습\r\npublic 서브넷 3기, private 서브넷 3기가 생성되어 있다.\r\nEKS 대시보드로 이동한다.\r\n생성된 클러스터의 모습\r\n생성된 노드 그룹의 모습\r\n노드 그룹 상세 화면이다.\r\n인스턴스의 크기가 m5.large 로 비교적 크다. 이 사이즈를 다시 작은 사이즈의 노드그룹으로 바꿔줘야 추가적인 비용과금을 최소화할 수 있다. 다음 문서에서는 이 m5.large 인스턴스를 t3.small 인스턴스로 교체하는 작업을 수행한다."}},"/0-setup-access-tools/0.5":{"title":"5. ArgoCD ALB 용도의 Ingress 에 서브넷 지정, 보안 그룹 생성","data":{}},"/1-create-an-eks/1.5":{"title":"1.5","data":{"5-oidc-조회-및-클러스터-service-account-생성#5. oidc 조회 및 클러스터 service account 생성":"AWS Loadbalancer Controller 의 공식 깃헙 블로그는 kubernetes-sigs.github.io/aws-load-balancer-controller 이다.\n로드밸런서 컨트롤러를 설정하기 위해서 가장 먼저 확인해야 할 내용은 Open Id Connect 가 쿠버네티스와 잘 연결되어 사용되고 있는지이다.\n쿠버네티스는 인그레스나 이런 것들을 배포를 하고 나면 Amazon 의 ALB 등을 제어를 해야 하는데, 이렇게 AWS 에 존재하는 ALB등을 제어하기 위해서 인증 방식의 다양한 방법 들 중 OAuth 방식의 Open Id Connect 를 지원해주고 있다.\n참고\nOpen ID(OIDC) 의 개념과 동작원리\n편의성을 높인 ID 인증관리 - OIDC(Open ID Connect) 가 주목받는 이유","환경변수-선언#환경변수 선언":"cluster_name , oidc 선언\n$ export cluster_name=gitops-study-k8scluster\r\n\r\n## eks cluster 에 대한 oidc_id 를 조회한다.\r\n$ oidc_id=$(aws eks describe-cluster --name $cluster_name --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5)\r\n\r\n## 구한 eks cluster 에 대한 oidc 를 출력해본다. \r\n$ echo $oidc_id\r\n8889A26B47C0F157228D712D894CA629\n생성한 oidc_id 는 필요할 때마다 아래의 명령을 수행해도 되지만 복사해서 따로 보관해두면 편하다.\n$ aws eks describe-cluster --name $cluster_name --query \"cluster.identity.oidc.issuer\" --output text | cut -d '/' -f 5\n만약 출력된 결과가 없다면 oidc 가 없다는 것인데, 아래의 명령을 통해 OIDC 자격증명 공급자를 생성해야 한다.\n$ eksctl utils associate-iam-oidc-provider --cluster $cluster_name --approve\r\n2023-12-24 13:56:17 [ℹ]  will create IAM Open ID Connect provider for cluster \"gitops-study-k8scluster\" in \"ap-northeast-2\"\r\n2023-12-24 13:56:17 [✔]  created IAM Open ID Connect provider for cluster \"gitops-study-k8scluster\" in \"ap-northeast-2\"\n내 경우에는 cluster 에 대한 iamserviceaccount 를 생성시에 오류가 났고, 위의 명령어를 그대로 수행하라는 출력이 나타났다. 그래서 위의 명령어를 수행한 후에 iamserviceaccount 를 다시 생성했다.","iam-정책-json-다운로드--aws-cli로-정책-생성#IAM 정책 json 다운로드 & AWS CLI로 정책 생성":"kubernetes 공식 깃헙 계정(github.com/kubernetes-sigs) 내의 리포지터리인 github.com/kubernetes-sigs/aws-load-balancer-controller 에서는 kubernetes 에서 AWS 환경에 맞도록 Ingress Controller 를 설치하기 위해 aws-load-balancer-controller 를 구성할 때 필요한 IAM Policy 부터 여러가지 다양한 AWS 환경 내에서의 리소스 Json 및 예제 등을 제공하고 있다.\n아래 iam_policy.json 은 위에서 공식 깃헙 계정 내의 https://github.com/kubernetes-sigs/aws-load-balancer-controller/blob/main/docs/install/iam_policy.json 에서 제공하는 IAM 정책 json 파일이다.\nCloud9 에서 다 귀찮다 싶으면 raw 파일 링크 을 통해 curl 로 다운받아서 사용하면 된다.\n{\r\n    \"Version\": \"2012-10-17\",\r\n    \"Statement\": [\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"iam:CreateServiceLinkedRole\"\r\n            ],\r\n            \"Resource\": \"*\",\r\n            \"Condition\": {\r\n                \"StringEquals\": {\r\n                    \"iam:AWSServiceName\": \"elasticloadbalancing.amazonaws.com\"\r\n                }\r\n            }\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"ec2:DescribeAccountAttributes\",\r\n                \"ec2:DescribeAddresses\",\r\n                \"ec2:DescribeAvailabilityZones\",\r\n                \"ec2:DescribeInternetGateways\",\r\n                \"ec2:DescribeVpcs\",\r\n                \"ec2:DescribeVpcPeeringConnections\",\r\n                \"ec2:DescribeSubnets\",\r\n                \"ec2:DescribeSecurityGroups\",\r\n                \"ec2:DescribeInstances\",\r\n                \"ec2:DescribeNetworkInterfaces\",\r\n                \"ec2:DescribeTags\",\r\n                \"ec2:GetCoipPoolUsage\",\r\n                \"ec2:DescribeCoipPools\",\r\n                \"elasticloadbalancing:DescribeLoadBalancers\",\r\n                \"elasticloadbalancing:DescribeLoadBalancerAttributes\",\r\n                \"elasticloadbalancing:DescribeListeners\",\r\n                \"elasticloadbalancing:DescribeListenerCertificates\",\r\n                \"elasticloadbalancing:DescribeSSLPolicies\",\r\n                \"elasticloadbalancing:DescribeRules\",\r\n                \"elasticloadbalancing:DescribeTargetGroups\",\r\n                \"elasticloadbalancing:DescribeTargetGroupAttributes\",\r\n                \"elasticloadbalancing:DescribeTargetHealth\",\r\n                \"elasticloadbalancing:DescribeTags\"\r\n            ],\r\n            \"Resource\": \"*\"\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"cognito-idp:DescribeUserPoolClient\",\r\n                \"acm:ListCertificates\",\r\n                \"acm:DescribeCertificate\",\r\n                \"iam:ListServerCertificates\",\r\n                \"iam:GetServerCertificate\",\r\n                \"waf-regional:GetWebACL\",\r\n                \"waf-regional:GetWebACLForResource\",\r\n                \"waf-regional:AssociateWebACL\",\r\n                \"waf-regional:DisassociateWebACL\",\r\n                \"wafv2:GetWebACL\",\r\n                \"wafv2:GetWebACLForResource\",\r\n                \"wafv2:AssociateWebACL\",\r\n                \"wafv2:DisassociateWebACL\",\r\n                \"shield:GetSubscriptionState\",\r\n                \"shield:DescribeProtection\",\r\n                \"shield:CreateProtection\",\r\n                \"shield:DeleteProtection\"\r\n            ],\r\n            \"Resource\": \"*\"\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"ec2:AuthorizeSecurityGroupIngress\",\r\n                \"ec2:RevokeSecurityGroupIngress\"\r\n            ],\r\n            \"Resource\": \"*\"\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"ec2:CreateSecurityGroup\"\r\n            ],\r\n            \"Resource\": \"*\"\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"ec2:CreateTags\"\r\n            ],\r\n            \"Resource\": \"arn:aws:ec2:*:*:security-group/*\",\r\n            \"Condition\": {\r\n                \"StringEquals\": {\r\n                    \"ec2:CreateAction\": \"CreateSecurityGroup\"\r\n                },\r\n                \"Null\": {\r\n                    \"aws:RequestTag/elbv2.k8s.aws/cluster\": \"false\"\r\n                }\r\n            }\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"ec2:CreateTags\",\r\n                \"ec2:DeleteTags\"\r\n            ],\r\n            \"Resource\": \"arn:aws:ec2:*:*:security-group/*\",\r\n            \"Condition\": {\r\n                \"Null\": {\r\n                    \"aws:RequestTag/elbv2.k8s.aws/cluster\": \"true\",\r\n                    \"aws:ResourceTag/elbv2.k8s.aws/cluster\": \"false\"\r\n                }\r\n            }\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"ec2:AuthorizeSecurityGroupIngress\",\r\n                \"ec2:RevokeSecurityGroupIngress\",\r\n                \"ec2:DeleteSecurityGroup\"\r\n            ],\r\n            \"Resource\": \"*\",\r\n            \"Condition\": {\r\n                \"Null\": {\r\n                    \"aws:ResourceTag/elbv2.k8s.aws/cluster\": \"false\"\r\n                }\r\n            }\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"elasticloadbalancing:CreateLoadBalancer\",\r\n                \"elasticloadbalancing:CreateTargetGroup\"\r\n            ],\r\n            \"Resource\": \"*\",\r\n            \"Condition\": {\r\n                \"Null\": {\r\n                    \"aws:RequestTag/elbv2.k8s.aws/cluster\": \"false\"\r\n                }\r\n            }\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"elasticloadbalancing:CreateListener\",\r\n                \"elasticloadbalancing:DeleteListener\",\r\n                \"elasticloadbalancing:CreateRule\",\r\n                \"elasticloadbalancing:DeleteRule\"\r\n            ],\r\n            \"Resource\": \"*\"\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"elasticloadbalancing:AddTags\",\r\n                \"elasticloadbalancing:RemoveTags\"\r\n            ],\r\n            \"Resource\": [\r\n                \"arn:aws:elasticloadbalancing:*:*:targetgroup/*/*\",\r\n                \"arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*\",\r\n                \"arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*\"\r\n            ],\r\n            \"Condition\": {\r\n                \"Null\": {\r\n                    \"aws:RequestTag/elbv2.k8s.aws/cluster\": \"true\",\r\n                    \"aws:ResourceTag/elbv2.k8s.aws/cluster\": \"false\"\r\n                }\r\n            }\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"elasticloadbalancing:AddTags\",\r\n                \"elasticloadbalancing:RemoveTags\"\r\n            ],\r\n            \"Resource\": [\r\n                \"arn:aws:elasticloadbalancing:*:*:listener/net/*/*/*\",\r\n                \"arn:aws:elasticloadbalancing:*:*:listener/app/*/*/*\",\r\n                \"arn:aws:elasticloadbalancing:*:*:listener-rule/net/*/*/*\",\r\n                \"arn:aws:elasticloadbalancing:*:*:listener-rule/app/*/*/*\"\r\n            ]\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"elasticloadbalancing:ModifyLoadBalancerAttributes\",\r\n                \"elasticloadbalancing:SetIpAddressType\",\r\n                \"elasticloadbalancing:SetSecurityGroups\",\r\n                \"elasticloadbalancing:SetSubnets\",\r\n                \"elasticloadbalancing:DeleteLoadBalancer\",\r\n                \"elasticloadbalancing:ModifyTargetGroup\",\r\n                \"elasticloadbalancing:ModifyTargetGroupAttributes\",\r\n                \"elasticloadbalancing:DeleteTargetGroup\"\r\n            ],\r\n            \"Resource\": \"*\",\r\n            \"Condition\": {\r\n                \"Null\": {\r\n                    \"aws:ResourceTag/elbv2.k8s.aws/cluster\": \"false\"\r\n                }\r\n            }\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"elasticloadbalancing:AddTags\"\r\n            ],\r\n            \"Resource\": [\r\n                \"arn:aws:elasticloadbalancing:*:*:targetgroup/*/*\",\r\n                \"arn:aws:elasticloadbalancing:*:*:loadbalancer/net/*/*\",\r\n                \"arn:aws:elasticloadbalancing:*:*:loadbalancer/app/*/*\"\r\n            ],\r\n            \"Condition\": {\r\n                \"StringEquals\": {\r\n                    \"elasticloadbalancing:CreateAction\": [\r\n                        \"CreateTargetGroup\",\r\n                        \"CreateLoadBalancer\"\r\n                    ]\r\n                },\r\n                \"Null\": {\r\n                    \"aws:RequestTag/elbv2.k8s.aws/cluster\": \"false\"\r\n                }\r\n            }\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"elasticloadbalancing:RegisterTargets\",\r\n                \"elasticloadbalancing:DeregisterTargets\"\r\n            ],\r\n            \"Resource\": \"arn:aws:elasticloadbalancing:*:*:targetgroup/*/*\"\r\n        },\r\n        {\r\n            \"Effect\": \"Allow\",\r\n            \"Action\": [\r\n                \"elasticloadbalancing:SetWebAcl\",\r\n                \"elasticloadbalancing:ModifyListener\",\r\n                \"elasticloadbalancing:AddListenerCertificates\",\r\n                \"elasticloadbalancing:RemoveListenerCertificates\",\r\n                \"elasticloadbalancing:ModifyRule\"\r\n            ],\r\n            \"Resource\": \"*\"\r\n        }\r\n    ]\r\n}\n이번 실습에서는 curl 을 통해서 IAM 정책을 적용하기로 했다.\r\n메인 브랜치 IAM Policy json 링크\nhttps://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json\nv2.5.4 브랜치 IAM Policy json 링크\nhttps://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.5.4/docs/install/iam_policy.json\n나는 메인 브랜치의 IAM Policy Json 을 사용하기로 했다.\n## iam policy 다운로드\r\n$ curl -O https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json\r\n  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\r\n                                 Dload  Upload   Total   Spent    Left  Speed\r\n100  8386  100  8386    0     0  28407      0 --:--:-- --:--:-- --:--:-- 28523\r\n\r\n## 다운로드 받은 json 파일인 iam_policy.json 을 이용해서 IAM Policy 생성 \r\n$ aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json\r\n\r\n실행결과\r\n{\r\n    \"Policy\": {\r\n        \"PolicyName\": \"AWSLoadBalancerControllerIAMPolicy\",\r\n        \"PolicyId\": \"ANPARSRDIIP7ERQMT63QE\",\r\n        \"Arn\": \"arn:aws:iam::{IAM 계정 ID}:policy/AWSLoadBalancerControllerIAMPolicy\",\r\n        \"Path\": \"/\",\r\n        \"DefaultVersionId\": \"v1\",\r\n        \"AttachmentCount\": 0,\r\n        \"PermissionsBoundaryUsageCount\": 0,\r\n        \"IsAttachable\": true,\r\n        \"CreateDate\": \"2023-12-24T12:55:42+00:00\",\r\n        \"UpdateDate\": \"2023-12-24T12:55:42+00:00\"\r\n    }\r\n}","service-account-생성#Service Account 생성":"$ eksctl create iamserviceaccount \\\r\n> --cluster=gitops-study-k8scluster \\\r\n> --namespace=kube-system \\\r\n> --name=aws-load-balancer-controller \\\r\n> --role-name AmazoneEKSLoadBalancerControllerRole \\\r\n> --attach-policy-arn=arn:aws:iam::{IAM 계정 ID}:policy/AWSLoadBalancerControllerIAMPolicy \\\r\n> --approve\n결과를 확인해본다.\n$ aws sts get-caller-identity\r\n{\r\n    \"UserId\": \"AIDARSRDIIP7JGE4QNBL7\",\r\n    \"Account\": \"108521866238\",\r\n    \"Arn\": \"arn:aws:iam::108521866238:user/gitops-study-argocd\"\r\n}\n생성된 service account 를 확인해본다.\n$ kubectl describe sa aws-load-balancer-controller -n kube-system\r\nName:                aws-load-balancer-controller\r\nNamespace:           kube-system\r\nLabels:              app.kubernetes.io/managed-by=eksctl\r\nAnnotations:         eks.amazonaws.com/role-arn: arn:aws:iam::108521866238:role/AmazoneEKSLoadBalancerControllerRole\r\nImage pull secrets:  <none>\r\nMountable secrets:   <none>\r\nTokens:              <none>\r\nEvents:              <none>\r\n\r\n## 또는 아래와 같이 serviceaccount 를 모두 적어주어도 된다.\r\n$ kubectl describe serviceaccount aws-load-balancer-controller -n kube-system                                         \r\nName:                aws-load-balancer-controller\r\nNamespace:           kube-system\r\nLabels:              app.kubernetes.io/managed-by=eksctl\r\nAnnotations:         eks.amazonaws.com/role-arn: arn:aws:iam::108521866238:role/AmazoneEKSLoadBalancerControllerRole\r\nImage pull secrets:  <none>\r\nMountable secrets:   <none>\r\nTokens:              <none>\r\nEvents:              <none>"}},"/1-create-an-eks/1.6":{"title":"1.6","data":{"6-helm-을-이용해-aws-load-balancer-controller-생성#6. helm 을 이용해 aws-load-balancer-controller 생성":"","cloud9-에-helm-설치#Cloud9 에 helm 설치":"helm 설치에 대한 명령어는 helm.sh - Installing Helm ## From Script 의 내용을 발췌해왔다.\r\nALB Ingress Controller 를 helm 을 이용해서 설치 예정이다. 따라서 Cloud9 에 helm 을 설치해야 한다.\n## helm.sh 라는 이름으로 헬름 설치 파일을 curl 로 다운로드\r\n$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\r\n\r\n## 다운로드 받은 파일 확안\r\n$ ls\r\nREADME.md  download  get_helm.sh  minimum-cluster.yml\r\n\r\n## 실행 권한 변경\r\n$ chmod 0700 get_helm.sh\r\n\r\n## 설치파일 실행\r\n$ ./get_helm.sh \r\nDownloading https://get.helm.sh/helm-v3.13.1-linux-amd64.tar.gz\r\nVerifying checksum... Done.\r\nPreparing to install helm into /usr/local/bin\r\nhelm installed into /usr/local/bin/helm","helm-을-이용해-aws-load-balancer-controller-설치#helm 을 이용해 aws-load-balancer-controller 설치":"$ helm repo add eks https://aws.github.io/eks-charts\r\n\"eks\" has been added to your repositories\r\n\r\n$ helm repo update eks\r\nHang tight while we grab the latest from your chart repositories...\r\n...Successfully got an update from the \"eks\" chart repository\r\nUpdate Complete. ⎈Happy Helming!⎈\r\n\r\n$ helm install aws-load-balancer-controller eks/aws-load-balancer-controller -n kube-system --set clusterName=gitops-study-k8scluster --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller\r\nNAME: aws-load-balancer-controller\r\nLAST DEPLOYED: Sun Dec 24 14:11:01 2023\r\nNAMESPACE: kube-system\r\nSTATUS: deployed\r\nREVISION: 1\r\nTEST SUITE: None\r\nNOTES:\r\nAWS Load Balancer controller installed!\r\n\r\n\r\n## 설치 확인\r\n## kube-system 네임스페이스 아래에 aws-load-balancer-controller 가 존재하는지 찾아본다.\r\n$ kubectl get deployment -n kube-system aws-load-balancer-controller\r\nNAME                           READY   UP-TO-DATE   AVAILABLE   AGE\r\naws-load-balancer-controller   2/2     2            2           104s"}},"/2-setup-argocd/2.2":{"title":"2.2","data":{"2-argocd-구축-2-argocd-에-http-허용#2. ArgoCD 구축 (2) ArgoCD 에 HTTP 허용":"Security Rule 수정\n80 포트 nodeport 생성 (service.yml 작성 → apply)\n80 포트 Rule 이 적용된 Security Rule Id를 명시한 ingress 생성 (ingress.yml 작성 → apply)\nargocd-server 파드의 경우 기본적으로 HTTP 요청 수신 시에는 HTTPS로 Redirect 되게끔 설정되어 있다. 현재 실습에서는 ArgoCD 서버에 80 포트를 허용하는 실습으로 진행하기로 했다. HTTPS 를 허용하려면 인증서를 발급받아야 하는데, 인프라 레벨에서 인증서를 등록하려면 비용문제도 있고, 개발 스터디 버전으로 인증서를 발급받아서 사용할 필요 까지는 없기 때문.\r\n혹시 만약 https 를 허용하려면 ALB가 AWS ACM 을 통해 SSL 인증서 동작을 처리하고 백엔드 애플리케이션으로는 HTTP 트래픽을 전달하게끔 해주면 된다.","argocd-deployment-내의-args에---insecure-옵션-추가-후-재배포#ArgoCD Deployment 내의 args에 --insecure 옵션 추가 후 재배포":"deployment 중 argocd-server 에 대해 아래의 JSON 옵션을 적용해서 patch 해준다.\r\n그리고 deployment 를 통해 배포된 pod 들이 실제로 재배포 되었는지 AGE 를 통해 체크한다.\n[\r\n  {\r\n    \"op\": \"replace\",\r\n    \"path\": \"/spec/template/spec/containers/0/args\",\r\n    \"value\": [\"/usr/local/bin/argocd-server\", \"--insecure\"]\r\n  }\r\n]\n위의 JSON 은 jsoneditoronline.org 을 이용해서 인라인 문자열로 변환해서 CLI 명령에 인자값으로 전달해줬다.\n실제 실습과정은 아래와 같다.\n## 네임스페이스 'argocd' 내에 deployment 가 어떤 것들이 있는지 조회\r\n$ kubectl get deployment -n argocd\r\nNAME                               READY   UP-TO-DATE   AVAILABLE   AGE\r\nargocd-applicationset-controller   1/1     1            1           62m\r\nargocd-dex-server                  1/1     1            1           62m\r\nargocd-notifications-controller    1/1     1            1           62m\r\nargocd-redis                       1/1     1            1           62m\r\nargocd-repo-server                 1/1     1            1           62m\r\nargocd-server                      1/1     1            1           62m\r\n\r\n\r\n## --insecure 옵션을 붙여서 argocd-server deployment 를 patch 하기\r\n$ kubectl -n argocd patch deployment argocd-server --type json -p='[{\"op\":\"replace\",\"path\":\"/spec/template/spec/containers/0/args\",\"value\":[\"/usr/local/bin/argocd-server\",\"--insecure\"]}]'\r\ndeployment.apps/argocd-server patched\r\n\r\n## 실제로 pod 이 재배포 되었는지 확인\r\n## 37초 전에 재배포 되었음을 확인 가능 \r\n## (이 글을 쓸때 잠깐 다른 것을 하다가 아래 명령을 내려서 37초 전이라고 표시되었다.)\r\n\r\n$ kubectl get po -n argocd\r\nNAME                                               READY   STATUS    RESTARTS   AGE\r\nargocd-application-controller-0                    1/1     Running   0          70m\r\nargocd-applicationset-controller-dc5c4c965-qrz7m   1/1     Running   0          70m\r\nargocd-dex-server-9769d6499-tl5jd                  1/1     Running   0          70m\r\nargocd-notifications-controller-db4f975f8-dhcqj    1/1     Running   0          70m\r\nargocd-redis-b5d6bf5f5-d75xc                       1/1     Running   0          70m\r\nargocd-repo-server-579cdc7849-6htrl                1/1     Running   0          70m\r\nargocd-server-5b59f8cc5c-pnd9q                     1/1     Running   0          37s\r\n\r\n## 배포된 deployment 인 'argocd-server' 의 실제 옵션을 확인해본다.\r\n$ kubectl -n argocd describe deployment argocd-server\r\n## 또는 단축어인 deploy 라는 리소스명으로 확인하는 것 역시 가능하다.\r\n$ kubectl -n argocd describe deploy argocd-server\r\n\r\nName:                   argocd-server\r\nNamespace:              argocd\r\nCreationTimestamp:      Mon, 25 Dec 2023 04:18:06 +0000\r\nLabels:                 app.kubernetes.io/component=server\r\n                        app.kubernetes.io/name=argocd-server\r\n                        app.kubernetes.io/part-of=argocd\r\nAnnotations:            deployment.kubernetes.io/revision: 2\r\nSelector:               app.kubernetes.io/name=argocd-server\r\nReplicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable\r\nStrategyType:           RollingUpdate\r\nMinReadySeconds:        0\r\nRollingUpdateStrategy:  25% max unavailable, 25% max surge\r\nPod Template:\r\n  Labels:           app.kubernetes.io/name=argocd-server\r\n  Service Account:  argocd-server\r\n  Containers:\r\n   argocd-server:\r\n    Image:           quay.io/argoproj/argocd:v2.9.3\r\n    Ports:           8080/TCP, 8083/TCP\r\n    Host Ports:      0/TCP, 0/TCP\r\n    SeccompProfile:  RuntimeDefault\r\n    Args:\r\n      /usr/local/bin/argocd-server\r\n\t\t\t## 실제로 이 부분을 보면 --insecure 가 적용되었음을 확인 가능하다.\r\n      --insecure\r\n    Liveness:   http-get http://:8080/healthz%3Ffull=true delay=3s timeout=5s period=30s #success=1 #failure=3\r\n    Readiness:  http-get http://:8080/healthz delay=3s timeout=1s period=30s #success=1 #failure=3\r\n    Environment:\r\n      ARGOCD_SERVER_INSECURE:                            <set to the key 'server.insecure' of config map 'argocd-cmd-params-cm'>                            Optional: true\r\n      ARGOCD_SERVER_BASEHREF:                            <set to the key 'server.basehref' of config map 'argocd-cmd-params-cm'>                            Optional: true\r\n      ARGOCD_SERVER_ROOTPATH:                            <set to the key 'server.rootpath' of config map 'argocd-cmd-params-cm'>                            Optional: true\r\n      ARGOCD_SERVER_LOGFORMAT:                           <set to the key 'server.log.format' of config map 'argocd-cmd-params-cm'>                          Optional: true\r\n      ARGOCD_SERVER_LOG_LEVEL:                           <set to the key 'server.log.level' of config map 'argocd-cmd-params-cm'>                           Optional: true\r\n      ARGOCD_SERVER_REPO_SERVER:                         <set to the key 'repo.server' of config map 'argocd-cmd-params-cm'>                                Optional: true\r\n      ARGOCD_SERVER_DEX_SERVER:                          <set to the key 'server.dex.server' of config map 'argocd-cmd-params-cm'>                          Optional: true\r\n      ARGOCD_SERVER_DISABLE_AUTH:                        <set to the key 'server.disable.auth' of config map 'argocd-cmd-params-cm'>                        Optional: true\r\n      ARGOCD_SERVER_ENABLE_GZIP:                         <set to the key 'server.enable.gzip' of config map 'argocd-cmd-params-cm'>                         Optional: true\r\n      ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS:         <set to the key 'server.repo.server.timeout.seconds' of config map 'argocd-cmd-params-cm'>         Optional: true\r\n      ARGOCD_SERVER_X_FRAME_OPTIONS:                     <set to the key 'server.x.frame.options' of config map 'argocd-cmd-params-cm'>                     Optional: true\r\n      ARGOCD_SERVER_CONTENT_SECURITY_POLICY:             <set to the key 'server.content.security.policy' of config map 'argocd-cmd-params-cm'>             Optional: true\r\n      ARGOCD_SERVER_REPO_SERVER_PLAINTEXT:               <set to the key 'server.repo.server.plaintext' of config map 'argocd-cmd-params-cm'>               Optional: true\r\n      ARGOCD_SERVER_REPO_SERVER_STRICT_TLS:              <set to the key 'server.repo.server.strict.tls' of config map 'argocd-cmd-params-cm'>              Optional: true\r\n      ARGOCD_SERVER_DEX_SERVER_PLAINTEXT:                <set to the key 'server.dex.server.plaintext' of config map 'argocd-cmd-params-cm'>                Optional: true\r\n      ARGOCD_SERVER_DEX_SERVER_STRICT_TLS:               <set to the key 'server.dex.server.strict.tls' of config map 'argocd-cmd-params-cm'>               Optional: true\r\n      ARGOCD_TLS_MIN_VERSION:                            <set to the key 'server.tls.minversion' of config map 'argocd-cmd-params-cm'>                      Optional: true\r\n      ARGOCD_TLS_MAX_VERSION:                            <set to the key 'server.tls.maxversion' of config map 'argocd-cmd-params-cm'>                      Optional: true\r\n      ARGOCD_TLS_CIPHERS:                                <set to the key 'server.tls.ciphers' of config map 'argocd-cmd-params-cm'>                         Optional: true\r\n      ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION:  <set to the key 'server.connection.status.cache.expiration' of config map 'argocd-cmd-params-cm'>  Optional: true\r\n      ARGOCD_SERVER_OIDC_CACHE_EXPIRATION:               <set to the key 'server.oidc.cache.expiration' of config map 'argocd-cmd-params-cm'>               Optional: true\r\n      ARGOCD_SERVER_LOGIN_ATTEMPTS_EXPIRATION:           <set to the key 'server.login.attempts.expiration' of config map 'argocd-cmd-params-cm'>           Optional: true\r\n      ARGOCD_SERVER_STATIC_ASSETS:                       <set to the key 'server.staticassets' of config map 'argocd-cmd-params-cm'>                        Optional: true\r\n      ARGOCD_APP_STATE_CACHE_EXPIRATION:                 <set to the key 'server.app.state.cache.expiration' of config map 'argocd-cmd-params-cm'>          Optional: true\r\n      REDIS_SERVER:                                      <set to the key 'redis.server' of config map 'argocd-cmd-params-cm'>                               Optional: true\r\n      REDIS_COMPRESSION:                                 <set to the key 'redis.compression' of config map 'argocd-cmd-params-cm'>                          Optional: true\r\n      REDISDB:                                           <set to the key 'redis.db' of config map 'argocd-cmd-params-cm'>                                   Optional: true\r\n      ARGOCD_DEFAULT_CACHE_EXPIRATION:                   <set to the key 'server.default.cache.expiration' of config map 'argocd-cmd-params-cm'>            Optional: true\r\n      ARGOCD_MAX_COOKIE_NUMBER:                          <set to the key 'server.http.cookie.maxnumber' of config map 'argocd-cmd-params-cm'>               Optional: true\r\n      ARGOCD_SERVER_LISTEN_ADDRESS:                      <set to the key 'server.listen.address' of config map 'argocd-cmd-params-cm'>                      Optional: true\r\n      ARGOCD_SERVER_METRICS_LISTEN_ADDRESS:              <set to the key 'server.metrics.listen.address' of config map 'argocd-cmd-params-cm'>              Optional: true\r\n      ARGOCD_SERVER_OTLP_ADDRESS:                        <set to the key 'otlp.address' of config map 'argocd-cmd-params-cm'>                               Optional: true\r\n      ARGOCD_APPLICATION_NAMESPACES:                     <set to the key 'application.namespaces' of config map 'argocd-cmd-params-cm'>                     Optional: true\r\n      ARGOCD_SERVER_ENABLE_PROXY_EXTENSION:              <set to the key 'server.enable.proxy.extension' of config map 'argocd-cmd-params-cm'>              Optional: true\r\n    Mounts:\r\n      /app/config/dex/tls from argocd-dex-server-tls (rw)\r\n      /app/config/server/tls from argocd-repo-server-tls (rw)\r\n      /app/config/ssh from ssh-known-hosts (rw)\r\n      /app/config/tls from tls-certs (rw)\r\n      /home/argocd from plugins-home (rw)\r\n      /tmp from tmp (rw)\r\n  Volumes:\r\n   plugins-home:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     \r\n    SizeLimit:  <unset>\r\n   tmp:\r\n    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)\r\n    Medium:     \r\n    SizeLimit:  <unset>\r\n   ssh-known-hosts:\r\n    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n    Name:      argocd-ssh-known-hosts-cm\r\n    Optional:  false\r\n   tls-certs:\r\n    Type:      ConfigMap (a volume populated by a ConfigMap)\r\n    Name:      argocd-tls-certs-cm\r\n    Optional:  false\r\n   argocd-repo-server-tls:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  argocd-repo-server-tls\r\n    Optional:    true\r\n   argocd-dex-server-tls:\r\n    Type:        Secret (a volume populated by a Secret)\r\n    SecretName:  argocd-dex-server-tls\r\n    Optional:    true\r\nConditions:\r\n  Type           Status  Reason\r\n  ----           ------  ------\r\n  Available      True    MinimumReplicasAvailable\r\n  Progressing    True    NewReplicaSetAvailable\r\nOldReplicaSets:  argocd-server-557c4c6dff (0/0 replicas created)\r\nNewReplicaSet:   argocd-server-5b59f8cc5c (1/1 replicas created)\r\nEvents:          <none>\n참고) 커맨드라인에 JSON 데이터를 인라인 문자열로 변환할 때 유용한 도구JSON 데이터를 인라인 문자열로 변환할 때 사용한 도구는 jsoneditoronline.org 이다.","worker-node-들이-속한-security-rule-수정#Worker Node 들이 속한 Security Rule 수정":"EC2 대시보드로 이동한다. 그리고 EC2 들 중에서 eksctl 에 의해 생성된 node 들 중 하나의 체크박스를 클릭해서 세부 사항을 확인한다. 워커 노드들은 모두 같은 id 의 보안그룹(Security Group) 을 사용하고 있는 것을 확인 가능하다. 이 보안 그룹을 클릭해서 보안그룹 페이지로 넘어간다.\r\n보안그룹 페이지에서는 인바운드 규칙 편집 버튼을 클릭해서 인바운드 규칙 편집 페이지로 이동한다.\r\n인바운드 규칙 편집 페이지에서는 8080 포트에 대해 모든 트래픽을 허용하게끔 한다.\r\n\r\n규칙 저장 버튼을 클릭해 규칙을 저장한다.\n저장된 인바운드 규칙은 아래와 같이 확인 가능하다.","80-포트-nodeport-생성-serviceyml-작성--apply#80 포트 nodeport 생성 (service.yml 작성 → apply)":"argoCD에 대해 80 포트를 통해 접속을 허용하는 서비스를 정의해야 한다. 서비스(Service)란 deployment 를 통해 배포되어 있는 pod 들을 외부와 통신이 가능하도록 할 때 어떤 방식으로 노출해야 하는가를 정의하는 kubernetes 의 리소스를 의미한다.\r\n서비스(Service) 에는 대표적으로 Ingress, Nodeport 등이 있다.\r\nArgoCD 설치를 통해 이미 운영중인 argocd 의 service 들은 무엇들이 있는지 확인해보자.\n$ kubectl -n argocd get service\r\nNAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\r\nargocd-applicationset-controller          ClusterIP   10.100.117.173   <none>        7000/TCP,8080/TCP            175m\r\nargocd-dex-server                         ClusterIP   10.100.105.200   <none>        5556/TCP,5557/TCP,5558/TCP   175m\r\nargocd-metrics                            ClusterIP   10.100.159.217   <none>        8082/TCP                     175m\r\nargocd-notifications-controller-metrics   ClusterIP   10.100.149.169   <none>        9001/TCP                     175m\r\nargocd-redis                              ClusterIP   10.100.170.48    <none>        6379/TCP                     175m\r\nargocd-repo-server                        ClusterIP   10.100.139.2     <none>        8081/TCP,8084/TCP            175m\r\nargocd-server                             ClusterIP   10.100.96.73     <none>        80/TCP,443/TCP               175m\r\nargocd-server-metrics                     ClusterIP   10.100.29.0      <none>        8083/TCP                     175m\r\n\r\n## 또는 아래와 같이 svc 라는 단축 명령어로 확인 가능하다.\r\n$ kubectl -n argocd get svc\r\nNAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\r\nargocd-applicationset-controller          ClusterIP   10.100.117.173   <none>        7000/TCP,8080/TCP            176m\r\nargocd-dex-server                         ClusterIP   10.100.105.200   <none>        5556/TCP,5557/TCP,5558/TCP   176m\r\nargocd-metrics                            ClusterIP   10.100.159.217   <none>        8082/TCP                     176m\r\nargocd-notifications-controller-metrics   ClusterIP   10.100.149.169   <none>        9001/TCP                     176m\r\nargocd-redis                              ClusterIP   10.100.170.48    <none>        6379/TCP                     176m\r\nargocd-repo-server                        ClusterIP   10.100.139.2     <none>        8081/TCP,8084/TCP            176m\r\nargocd-server                             ClusterIP   10.100.96.73     <none>        80/TCP,443/TCP               176m\r\nargocd-server-metrics                     ClusterIP   10.100.29.0      <none>        8083/TCP                     176m\n이번에는 외부에서 워커노드(EC2)의 80 포트로의 유입이 발생할 경우 Pod 의 8080 포트와 연결해주는 NodePort 를 작성한다. yml 파일 명과 그 내용은 아래와 같다.\r\nargocd-server-nodeport.yml\n---\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  labels:\r\n    app: argocd-server-nodeport\r\n  name: argocd-server-nodeport\r\n  namespace: argocd\r\nspec:\r\n  ports:\r\n  - name: \"80\"\r\n    port: 80\r\n    targetPort: 8080\r\n    protocol: TCP\r\n  selector:\r\n    app.kubernetes.io/name: argocd-server\r\n  sessionAffinity: None\r\n  type: NodePort\n워커노드의 80 포트에 대한 트래픽을 Pod 의 8080 포트에 연결해주는 NodePort 에 대한 내용이다.\r\n그리고 이 yml 파일을 kubectl 로 적용하면 아래와 같다.\n## kubectl 을 통해 적용한다.\r\n$ kubectl apply -f argocd-server-nodeport.yml \r\nservice/argocd-server-nodeport created\r\n\r\n## 직접 service 를 조회해보면 아래와 같이 argocd-server-nodeport 가 정상적으로 추가되었음으 확인 가능하다.\r\n$ kubectl -n argocd get service\r\nNAME                                      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\r\nargocd-applicationset-controller          ClusterIP   10.100.117.173   <none>        7000/TCP,8080/TCP            4h44m\r\nargocd-dex-server                         ClusterIP   10.100.105.200   <none>        5556/TCP,5557/TCP,5558/TCP   4h44m\r\nargocd-metrics                            ClusterIP   10.100.159.217   <none>        8082/TCP                     4h44m\r\nargocd-notifications-controller-metrics   ClusterIP   10.100.149.169   <none>        9001/TCP                     4h44m\r\nargocd-redis                              ClusterIP   10.100.170.48    <none>        6379/TCP                     4h44m\r\nargocd-repo-server                        ClusterIP   10.100.139.2     <none>        8081/TCP,8084/TCP            4h44m\r\nargocd-server                             ClusterIP   10.100.96.73     <none>        80/TCP,443/TCP               4h44m\r\nargocd-server-metrics                     ClusterIP   10.100.29.0      <none>        8083/TCP                     4h44m\r\nargocd-server-nodeport                    NodePort    10.100.53.245    <none>        80:30040/TCP                 4m53s","80-포트-rule-이-적용된-security-rule-id-를-명시한-ingress-생성-ingressyml-작성--apply#80 포트 Rule 이 적용된 Security Rule ID 를 명시한 Ingress 생성 (ingress.yml 작성 → apply)":"아래 코드에 적용하는 security group, subnet 을 생성하는 과정은\n[TODO: ArgoCD ALB 용도의 Ingress 에 Subnet 지정, 보안그룹 생성] 에 따로 정리해두었다.\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: argocd\r\n  namespace: argocd\r\n  annotations:\r\n    alb.ingress.kubernetes.io/listen-ports: '[{\"HTTP\": 80}]'\r\n    alb.ingress.kubernetes.io/scheme: internet-facing\r\n    alb.ingress.kubernetes.io/target-type: ip\r\n    alb.ingress.kubernetes.io/healthcheck-path: /healthz\r\n    alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\r\n    alb.ingress.kubernetes.io/success-codes: '200'\r\n    alb.ingress.kubernetes.io/security-groups: sg-xxxxx\r\n    alb.ingress.kubernetes.io/subnets: subnet-xxxxx,subnet-xxxxx,subnet-xxxxx\r\nspec:\r\n  ingressClassName: alb\r\n  rules:\r\n  - http:\r\n      paths:\r\n      - path: /\r\n        backend:\r\n          service:\r\n            name: argocd-server-nodeport\r\n            port:\r\n              number: 80\r\n        pathType: Prefix\nALB 의 경우 외부와 통신을 통해 내부의 Nodeport 와 연결해줘야 하므로 ALB 자신은 Public 서브넷에 위치해야 한다. 이런 이유로 alb.ingress.kubernetes.io/subnets 에는 eksctl 이 생성한 Public 서브넷만 들만을 명시해줘야 한다.\n작성한 ekscluster-global-ingress.yml 파일을 kubectl 을 통해 적용한다.\n$ kubectl apply -f ekscluster-global-ingress.yml \r\ningress.networking.k8s.io/argocd created\r\n\r\n## ingress 조회\r\n$ kubectl -n argocd get ingress\r\nNAME     CLASS   HOSTS   ADDRESS                                                                   PORTS   AGE\r\nargocd   alb     *       k8s-argocd-argocd-23f933d87c-589378327.ap-northeast-2.elb.amazonaws.com   80      2m17s\r\n\r\n\r\n\r\n## ingress 세부 내용 조회\r\n$ kubectl -n argocd describe ingress argocd\r\nName:             argocd\r\nLabels:           <none>\r\nNamespace:        argocd\r\nAddress:          k8s-argocd-argocd-23f933d87c-589378327.ap-northeast-2.elb.amazonaws.com\r\nIngress Class:    alb\r\nDefault backend:  <default>\r\nRules:\r\n  Host        Path  Backends\r\n  ----        ----  --------\r\n  *           \r\n              /   argocd-server-nodeport:80 (192.168.124.124:8080)\r\nAnnotations:  alb.ingress.kubernetes.io/healthcheck-path: /healthz\r\n              alb.ingress.kubernetes.io/healthcheck-protocol: HTTP\r\n              alb.ingress.kubernetes.io/listen-ports: [{\"HTTP\": 80}]\r\n              alb.ingress.kubernetes.io/scheme: internet-facing\r\n              alb.ingress.kubernetes.io/security-groups: sg-0dfb95642ecc23d76\r\n              alb.ingress.kubernetes.io/subnets: subnet-06fe014719b653560,subnet-023fe2ec39f326241,subnet-0735aaf15bfeabec6\r\n              alb.ingress.kubernetes.io/success-codes: 200\r\n              alb.ingress.kubernetes.io/target-type: ip\r\nEvents:\r\n  Type    Reason                  Age   From     Message\r\n  ----    ------                  ----  ----     -------\r\n  Normal  SuccessfullyReconciled  3m5s  ingress  Successfully reconciled\nec2 대시보드에서 로드밸런서가 잘 생성되어있는지 확인해보자.\r\n리스너 및 규칙을 확인해보자.\r\n맨 아래의 리스너 및 규칙 탭 → HTTP:80 링크를 클릭한다.\r\n이동한 리스너 규칙 페이지에서는 대상 그룹으로 전달 에 해당하는 링크를 클릭한다.\r\n이동한 페이지에서는 리스너 규칙에 해당하는 대상 그룹에서의 트래픽 현환을 확인해볼 수 있다.\r\n밑으로 스크롤을 해보면 Healthy 라고 표시된 상태가 나타난다.\r\n이 Healthy 라고 하는 상태는 상태검사 탭에서 볼수 있듯 /heathz 에 대해 주기적으로 체크함으로써 얻어내는 상태값이다."}},"/2-setup-argocd/2.1":{"title":"2.1","data":{"1-argocd-공식-제공-yml-을-이용해-argocd-설치#1. ArgoCD 공식 제공 yml 을 이용해 ArgoCD 설치":"aregocd 설치 명령어는 argo-cd.readthedocs.io - getting started 에 자세하게 설명되어 있다.\n## argocd 관리를 위한 namespace 생성\r\n## 설치 전에 namespace 를 생성하지 않고, 설치 시에 namespace 를 지정하지 않으면\r\n## 나중에 리소스들을 삭제하거나 정리할 때 또는 관리할 때 곤란해진다.\r\n$ kubectl create namespace argocd\r\nnamespace/argocd created\r\n\r\n## argocd 생성\r\n$ kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\r\n## 굉장히 많은 요소들이 설치된다.\r\n## serviceaccount, rbac, rolebinding, configmap, secret, service, deployment, statefulset, networkpolicy\r\n\r\ncustomresourcedefinition.apiextensions.k8s.io/applications.argoproj.io created\r\ncustomresourcedefinition.apiextensions.k8s.io/applicationsets.argoproj.io created\r\ncustomresourcedefinition.apiextensions.k8s.io/appprojects.argoproj.io created\r\nserviceaccount/argocd-application-controller created\r\nserviceaccount/argocd-applicationset-controller created\r\nserviceaccount/argocd-dex-server created\r\nserviceaccount/argocd-notifications-controller created\r\nserviceaccount/argocd-redis created\r\nserviceaccount/argocd-repo-server created\r\nserviceaccount/argocd-server created\r\nrole.rbac.authorization.k8s.io/argocd-application-controller created\r\nrole.rbac.authorization.k8s.io/argocd-applicationset-controller created\r\nrole.rbac.authorization.k8s.io/argocd-dex-server created\r\nrole.rbac.authorization.k8s.io/argocd-notifications-controller created\r\nrole.rbac.authorization.k8s.io/argocd-server created\r\nclusterrole.rbac.authorization.k8s.io/argocd-application-controller created\r\nclusterrole.rbac.authorization.k8s.io/argocd-server created\r\nrolebinding.rbac.authorization.k8s.io/argocd-application-controller created\r\nrolebinding.rbac.authorization.k8s.io/argocd-applicationset-controller created\r\nrolebinding.rbac.authorization.k8s.io/argocd-dex-server created\r\nrolebinding.rbac.authorization.k8s.io/argocd-notifications-controller created\r\nrolebinding.rbac.authorization.k8s.io/argocd-server created\r\nclusterrolebinding.rbac.authorization.k8s.io/argocd-application-controller created\r\nclusterrolebinding.rbac.authorization.k8s.io/argocd-server created\r\nconfigmap/argocd-cm created\r\nconfigmap/argocd-cmd-params-cm created\r\nconfigmap/argocd-gpg-keys-cm created\r\nconfigmap/argocd-notifications-cm created\r\nconfigmap/argocd-rbac-cm created\r\nconfigmap/argocd-ssh-known-hosts-cm created\r\nconfigmap/argocd-tls-certs-cm created\r\nsecret/argocd-notifications-secret created\r\nsecret/argocd-secret created\r\nservice/argocd-applicationset-controller created\r\nservice/argocd-dex-server created\r\nservice/argocd-metrics created\r\nservice/argocd-notifications-controller-metrics created\r\nservice/argocd-redis created\r\nservice/argocd-repo-server created\r\nservice/argocd-server created\r\nservice/argocd-server-metrics created\r\ndeployment.apps/argocd-applicationset-controller created\r\ndeployment.apps/argocd-dex-server created\r\ndeployment.apps/argocd-notifications-controller created\r\ndeployment.apps/argocd-redis created\r\ndeployment.apps/argocd-repo-server created\r\ndeployment.apps/argocd-server created\r\nstatefulset.apps/argocd-application-controller created\r\nnetworkpolicy.networking.k8s.io/argocd-application-controller-network-policy created\r\nnetworkpolicy.networking.k8s.io/argocd-applicationset-controller-network-policy created\r\nnetworkpolicy.networking.k8s.io/argocd-dex-server-network-policy created\r\nnetworkpolicy.networking.k8s.io/argocd-notifications-controller-network-policy created\r\nnetworkpolicy.networking.k8s.io/argocd-redis-network-policy created\r\nnetworkpolicy.networking.k8s.io/argocd-repo-server-network-policy created\r\nnetworkpolicy.networking.k8s.io/argocd-server-network-policy created\n그리고 설치된 argocd 는 아래와 같이 확인해보자\n$ kubectl get po -n argocd\r\nNAME                                               READY   STATUS    RESTARTS   AGE\r\nargocd-application-controller-0                    1/1     Running   0          5m43s\r\nargocd-applicationset-controller-dc5c4c965-qrz7m   1/1     Running   0          5m43s\r\nargocd-dex-server-9769d6499-tl5jd                  1/1     Running   0          5m43s\r\nargocd-notifications-controller-db4f975f8-dhcqj    1/1     Running   0          5m43s\r\nargocd-redis-b5d6bf5f5-d75xc                       1/1     Running   0          5m43s\r\nargocd-repo-server-579cdc7849-6htrl                1/1     Running   0          5m43s\r\nargocd-server-557c4c6dff-px8zw                     1/1     Running   0          5m43s","에러-발생할-경우#에러 발생할 경우":"만약 아래와 같은 에러가 발생한다면\n$ kubectl create namespace argocd\r\nerror: You must be logged in to the server (Unauthorized)\n이전에 미리 만들어뒀던 export-acess-key-gitops-study-argocd.sh 을 활용해 Access Key 들을 OS 내에 export 해준다.\n$ source export-acess-key-gitops-study-argocd.sh"}},"/2-setup-argocd/2.3":{"title":"2.3","data":{"3-80포트-허용된-argocd-에-ingress-url-을-통해-접속해보기#3. 80포트 허용된 ArgoCD 에 Ingress URL 을 통해 접속해보기":"EC2 대시보드 → 로드밸런서 메뉴로 이동하면 아래와 같이 DNS 이름을 확인할 수 있다. 이 것을 복사한다.\r\n또는 Cloud 9 내에서 아래와 같이 kubectl -n argocd describe ingress argocd 명령을 통해 Address 에 해당하는 주소를 복사한다.\n$ kubectl -n argocd describe ingress argocd\r\nName:             argocd\r\nLabels:           <none>\r\nNamespace:        argocd\r\nAddress:          k8s-argocd-argocd-23f933d87c-589378327.ap-northeast-2.elb.amazonaws.com\r\nIngress Class:    alb\r\n\r\n// ...\n복사한 주소를 브라우저에 입력해서 이동하 아래와 같이 ArgoCD 로그인 페이지가 나타난다."}},"/2-setup-argocd/2.4":{"title":"2.4","data":{"4-argocd-의-default-password-변경#4. ArgoCD 의 Default Password 변경":""}},"/3-etc/3.2":{"title":"3.2","data":{"트러블슈팅-cloud9-에서-aws-credential-이-자주-refresh-되는-이슈#(트러블슈팅) Cloud9 에서 AWS Credential 이 자주 Refresh 되는 이슈":""}},"/3-etc/3.1":{"title":"3.1","data":{"주요-공식자료들#주요 공식자료들":""}},"/advanced":{"title":"Advanced","data":{"":"This is the index page for the Advanced folder!"}},"/about":{"title":"About","data":{"":"This is the about page! This page is shown on the navbar."}},"/":{"title":"Introduction","data":{"":"Welcome to Nextra! This is a basic docs template. You can use it as a starting point for your own project :)","what-is-nextrazxcv#What is Nextra?zxcv":"A simple, powerful and flexible site generation framework with everything you love from Next.js.","documentation#Documentation":"The documentation is available at https://nextra.site."}},"/advanced/satori":{"title":"Satori","data":{"":"Satori (悟り) is a Japanese Buddhist term for awakening, \"comprehension; understanding\"."}},"/another":{"title":"Another Page","data":{"":"let a = 1;\r\n\r\nconsole.log(a);","component#Component":"","external-component#External Component":""}}}